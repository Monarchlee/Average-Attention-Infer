# Average-Attention-Infer
The code and results in paper \textit{Consistent Focus: Mitigating Permutation Bias in Large Language Models through Attention Weight Averaging}.





## Quick Start

#### Setup

Requirements are simple, just transformer and pytorch.

```txt
# requirements
transformers>=4.41.3
torch==2.2.2+cu121
```

#### Replace the model code.

After installing packages, one should replace the model in transformers into AAT's version.

```
src/transformers/models/llama/modeling_llama.py <- Average-Attention-Infer/source
/models/modeling_llama_backup.py
```





#### Result file folder.

We set the results folder in 4 sub folders.

```tex
results
    |
    |--batch_pos_output
    |--batch_forward_output
    |--batch_p_output
    |--batch_text_output
    |--batch_pkl_output
```



#### Generate option positions.

To make one total experiment. One should first generate positions of commutative parts of datasets.

```bash
python generate_pos.py \\
    --model ../models/Llama-2/Llama-2-13b-chat-hf \\
    --device 0 \\
    --shot_num 0 \\ # for few-shot setting, if set non-zero then shot_path should be set
    --shot_path "" \\ # option
    --dataset Samsoup/cosmos_qa \\ # from huggingface
    --data_path '' \\ # load from local
    --test_ratio 0.1 \\ # split the dataset
    --shuffle 0 \\ # if shuffled dataset
    --test_name test \\ # result file name

```

After this, the generated position file will be saved in ../results/batch_pos_output.



#### Forward

Then forward AAT with prob_infer.py/text_infer.py.

```bash
python prob_infer.py \\
    --model ../models/Llama-2/Llama-2-13b-chat-hf \\
    --device 0 \\
    --if_lora 0 \\ # if enable lora adapter
    --shot_num 0 \\ # for few-shot setting, if set non-zero then shot_path should be set
    --shot_path "" \\ # option
    --dataset Samsoup/cosmos_qa \\ # from huggingface
    --data_path '' \\ # load from local
    --debias avg_att_infer_batch \\ # debias method
    --debias_layers 0 40 \\ # the debiased layer interval
    --debias_set 24 \\ # the size of permutation set
    --test_ratio 0.1 \\ # split the dataset
    --shuffle 0 \\ # if shuffled dataset
	--adapter_path '' \\ # enable when if_lora=1
	--pos_path ../results/batch_pos_output/test.json \\ # the position file path generated
	--test_name test \\ # result file name
```

After forward, the forward file will be saved in ../results/batch_forward_output.



#### Analysis

This stage  needs a cmp_file which indicates the original result of model. To generate the original result, see [preorder_prob_test.py](https://github.com/Monarchlee/Average-Attention-Infer/blob/main/source/prob/preorder_prob_test.py).

```bash
python analysis.py \\
    --dataset Samsoup/cosmos_qa \\ # from huggingface
    --data_path '' \\ # load from local
    --shot_num 0 \\ # for few-shot setting, if set non-zero then shot_path should be set
    --shot_path "" \\ # option
    --test_ratio 0.1 \\ # split the dataset
    --shuffle 0 \\ # if shuffled dataset
	--forward_path '' \\ # the forward result path
	--cmp_path '' \\ # the pkl file generated by other analysis.py or orginal file
	--test_name test \\ # result file name
```

This will generate .json file in ../results/batch_p_output and breakdown results in ../results/batch_pkl_output.



## Demo results

Results will be released later due to anonymous consideration.



